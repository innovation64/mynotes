---
{"dg-publish":true,"permalink":"/rl-column/hf-chapter2/2-7-q-learning/","dgPassFrontmatter":true,"noteIcon":"data:image/webp;base64,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"}
---



## 什么是Q-Learning？

Q-Learning是一种**异策略的基于价值**的方法，它**使用时序差分方法来训练其动作价值函数**：

- *异策略*：我们将在本单元的最后讨论这个问题。
- *基于价值的方法*：通过训练一个价值函数或动作价值函数来间接地找到最优策略，该函数能告诉我们**每个状态或每个状态-动作对的价值**。
- *使用时序差分方法*：**在每一步更新其动作价值函数，而不是在回合结束时进行更新。**

**Q-learning是我们用来训练 Q 函数的算法**，Q 函数是一个**动作价值函数**，用于确定在特定状态下采取特定动作的价值。

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg" alt="Q-function"/>
  <figcaption>给定一个状态和动作，Q函数将输出对应的状态-动作值(也叫做Q值)</figcaption>
</figure>
**Q来源于“Quality”（价值），即该状态下的动作价值。**

让我们回顾一下价值和奖励之间的区别：

- *状态的价值*或*状态-动作对的价值*是智能体在此状态（或状态-动作对）开始行动并按照其策略行事时预期的累积奖励。
- *奖励*是在状态下执行动作后**从环境中获得的反馈**。

在内部，Q 函数有一个**Q 表格，这个表中的每个单元格对应一个状态-动作对的价值。可以将这个 Q 表格视为 Q 函数的记忆或速查表。**

让我们通过一个迷宫的例子来解释以上内容。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-1.jpg" alt="Maze example"/>

我们对Q表格进行初始化，所以其中的值都为0. 这个表格**包含了每个状态的四个状态-动作值。**
对于这个简单的例子，状态仅由鼠标的位置定义。因此，在我们的Q表中我们有2*3行，每行对应鼠标可能的每一种位置。在更复杂的情况下，状态可能包含比行动者位置更多的信息。


<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-2.jpg" alt="Maze example"/>

在这里我们可以看到，**初始状态向上的状态-动作值为 0：**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-3.jpg" alt="Maze example"/>

因此，Q 函数包含一个 Q 表格，其中包含每个状态动作对的值。给定一个状态和动作，Q 函数会在其 Q 表格中搜索并输出该值。

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg" alt="Q-function"/>
</figure>


回顾一下，*Q-learning*是一个包含以下过程的**强化学习算法**: 

- 训练一个 *Q-函数*（一个**动作价值函数**），其内部是一个**包含所有状态-动作对值的Q表格。**
- 给定状态和动作，我们的Q函数**会在其 Q 表格中查找相应的值。**
- 当训练完成后，**我们有了一个最优的 Q-函数，这意味着我们有了最优的 Q 表格。**
- 如果我们**拥有最优的 Q 函数**，我们**拥有最优的策略**，因为我们**知道每个状态下应该采取的最佳动作。**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg" alt="Link value policy"/>

但是，在开始时**我们的Q表格是没有用的，因为它给每个状态-动作对赋予了任意的值**（大多数情况下，我们把 Q 表格初始化为 0）。随着智能体**探索环境并更新 Q 表格，它会给我们更好的近似最优策略。**

<figure class="image table text-center m-0 w-full">
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-1.jpg" alt="Q-learning"/>
  <figcaption>可以看到随着训练，Q 表格变得更好了，因为借助它，我们可以知道每个状态-动作对的值。</figcaption>
</figure>
现在我们已经理解了什么是 Q-learning，Q 函数和 Q 表格，接下来**让我们深入了解一下 Q-learning 算法**。

## Q-learning 算法

这是 Q-learning 的伪代码；让我们研究一下其中的每一部分，并在实现它之前**用一个简单的例子看看它是如何工作的。**不要被它的形式吓到，它比看起来简单！

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg" alt="Q-learning"/>

### 第一步: 初始化 Q 表格

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-3.jpg" alt="Q-learning"/>

我们需要初始化 Q 表格中的每个状态动作值。**大多数情况下，我们用 0 来初始化。**

### 第二步: 使用 epsilon 贪心策略选择一个动作

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg" alt="Q-learning"/>

epsilon-贪心策略是一种处理探索与利用权衡的策略。

其思想是首先定义初始 epsilon ɛ = 1.0：

- *概率 1 — ɛ*：智能体进行**利用**（即智能体选择具有最高状态-动作对值的动作）。
- 概率 ɛ：**智能体进行探索**（尝试随机动作）。

在训练开始时，**由于 ɛ 值很高，进行探索的概率会很大，所以智能体大部分时间都在探索。但随着训练的进行，Q表格在估计中越来越准确，所以逐渐降低 epsilon 值**，因为智能体逐渐不再需要探索，而需要更多地进行利用。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-5.jpg" alt="Q-learning"/>

### 第三步: 执行动作 At, 得到奖励 Rt+1 和下一个状态 St+1

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-6.jpg" alt="Q-learning"/>

### 第四步: 更新 Q(St, At)

需要注意的是，在时序差分学习中，我们在与环境交互之后更新策略或价值函数（取决于我们选择的强化学习方法）。

为了计算时序差分目标(TD target)，**我们使用即时奖励 \(R_{t+1}\) 加上下一个状态最佳状态-动作对的折扣价值**（称之为自举）。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-7.jpg" alt="Q-learning"/>

因此，(Q(S_t, A_t)\) **更新公式如下：**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-8.jpg" alt="Q-learning"/>

这意味着要更新 \(Q(S_t, A_t)\)：

- 需要 \(S_t, A_t, R_{t+1}, S_{t+1}\)。
- 需要更新给定状态-动作对的 Q 值，使用时序差分目标。

如何形成时序差分目标？

1. 在采取动作 \\(A_t\\) 后获得奖励 \\(R_{t+1}\\)。
2. 为了获得**最佳的下一个状态-动作对值**，使用贪心策略来选择下一个最佳动作。需要注意的是，这不是一个 ε-贪心策略，其将始终采取具有最高状态-动作值的动作。

然后，在此Q值更新完成后，将开始一个新的状态，并**再次使用 ε-贪心策略选择动作。**

**这就是为什么我们说Q学习是一种异策略算法。**

## 异策略 vs 同策略

它们之间只有细微的区别：

- *异策略*：**在行动（推断）和更新（训练）部分中**使用**不同的策略**。

例如，使用 Q 学习，ε-贪心策略（行动策略），与**用于选择最佳下一状态动作值来更新Q值（更新策略）**的贪心策略不同。

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-1.jpg" alt="Off-on policy"/>
  <figcaption>行动策略</figcaption>
</figure>



与我们在训练部分中所使用的策略不同：

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-2.jpg" alt="Off-on policy"/>
  <figcaption>更新策略</figcaption>
</figure>



- 同策略：在**行动和更新部分**中使用**相同的策略**。

例如，在另一种基于值的算法Sarsa中，**执行ε-贪心策略时，它选择的不是最优动作，而是下一个状态-动作对。**


<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-3.jpg" alt="Off-on policy"/>
    <figcaption>Sarsa</figcaption>
</figure>


<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg" alt="Off-on policy"/>
</figure>
